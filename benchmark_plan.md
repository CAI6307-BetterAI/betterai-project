# Benchmarking Plan â€“ Hallucination Reduction in Language Models via RDF Graphs & Jamba

## Objective
Evaluate and benchmark how RDF-grounded retrieval and Jamba (Mixture-of-Experts + RAG hybrid) architectures reduce hallucinations in large language models (LLMs) compared to standard baselines.

---

## Goals
1. Measure **hallucination reduction** achieved through RDF grounding and Jamba integration.
2. Benchmark **retrieval effectiveness**, **factual accuracy**, **latency**, and **coverage**.
3. Provide reproducible and transparent evaluation scripts, data, and metrics.

---

## Baseline Models

| Tier | Model | Type | Purpose |
|------|--------|------|---------|
| A | **LLaMA-3-8B-Instruct** | Vanilla LM | Measures raw hallucination tendency |
| B | **RAG-LLaMA-3-8B (Text)** | Text-retrieval RAG | Evaluates unstructured evidence |
| C | **RDF-RAG (SPARQL)** | Graph-grounded RAG | Measures effect of structured grounding |
| D | **Jamba-1.5 + RDF** | MoE + KG retrieval | Proposed advanced architecture |
| E | **GPT-3.5-Turbo / Human Gold** | Reference | Provides upper bound for factual accuracy |

---

## Dataset Preparation

### 1. Source Datasets
- **PubMedQA** or **MedQA**: medical Q&A dataset with verified answers.
- **Domain-specific RDF Graphs** (UMLS, Bio2RDF, custom KG).

### 2. Data Schema
```json
{
  "id": 1,
  "question": "What is the treatment for diabetes?",
  "gold": "Insulin therapy and lifestyle modification.",
  "triples": [
    ["Diabetes", "treated_by", "Insulin"],
    ["Diabetes", "treated_by", "Exercise"]
  ]
}
```

### 3. Size
Start with **100â€“500** questions for pilot; scale to **1000+** for final benchmark.

---

## âš™ï¸ Evaluation Pipeline Structure

```
evaluation/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prompts.jsonl
â”‚   â”œâ”€â”€ gold.jsonl
â”‚   â””â”€â”€ retrieved_triples.jsonl
â”‚
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ llama3_baseline/outputs.jsonl
â”‚   â”œâ”€â”€ rag_text/outputs.jsonl
â”‚   â”œâ”€â”€ rdf_rag/outputs.jsonl
â”‚   â””â”€â”€ jamba_rdf/outputs.jsonl
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ make_claims.py
â”‚   â”œâ”€â”€ nli_agreement.py
â”‚   â”œâ”€â”€ retrieval_scores.py
â”‚   â”œâ”€â”€ calibrate_risk.py
â”‚   â””â”€â”€ compute_metrics.py
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ summary_metrics.csv
    â”œâ”€â”€ hallucination_examples.md
    â””â”€â”€ plots/
```

---

## ðŸ§® Metrics & Definitions

| Metric | Description | Ideal Trend |
|---------|--------------|-------------|
| **Hallucination Rate (H)** | Fraction of unsupported / contradicted claims | â†“ Lower |
| **Factual Precision** | Supported claims Ã· total claims | â†‘ Higher |
| **ROUGE-L / F1** | Overlap with reference answers | â†‘ Higher |
| **Retrieval Precision@k** | % of correct triples retrieved | â†‘ Higher |
| **Latency** | Average response time (seconds) | â†“ Lower |
| **ECE / Brier Score** | Calibration quality of confidence gate | â†“ Lower |
| **Coverage** | % of questions answered (not abstained) | Moderate |

---

## Evaluation Steps

### Step 1 â€” Generate Outputs
Run all models on the same dataset:
```bash
python run_model.py --model llama3-8b --input data/prompts.jsonl --output runs/llama3_baseline/outputs.jsonl
python run_model.py --model rdf_rag --use_rdf True --output runs/rdf_rag/outputs.jsonl
```

### Step 2 â€” Compare with Evidence
Use **triple matching** or **LLM-as-Judge**:
```text
"Given these RDF triples and the model's answer, is it factually correct? (Yes/No)"
```

### Step 3 â€” Compute Metrics
Aggregate per-sample results:
```bash
python scripts/compute_metrics.py --input runs/ --output results/summary_metrics.csv
```

### Step 4 â€” Visualize
Produce comparative bar charts:
- Hallucination Rate vs Model
- Latency vs Accuracy
- Example â€œGood vs Hallucinatedâ€ answers

---

## Example Result Table

| Model | RDF | H â†“ | F1 â†‘ | Latency (s) â†“ | Notes |
|--------|-----|-----|-----|---------------|-------|
| LLaMA-3-8B | âŒ | 0.45 | 0.61 | 1.2 | Pure LM |
| RAG-Text | âŒ | 0.36 | 0.68 | 2.3 | Text retrieval |
| RDF-RAG | âœ… | 0.28 | 0.75 | 2.9 | Structured grounding |
| Jamba + RDF | âœ… | **0.22** | **0.79** | 3.1 | Best factual accuracy |

---

## Calibration & Risk Gating

To prevent hallucination:
1. Compute **risk score** = f(evidence agreement, retrieval quality, logit confidence, self-consistency).  
2. If risk > Ï„ â†’ abstain or re-retrieve.  
3. Tune Ï„ on dev set to balance factuality vs coverage.

Evaluate with Expected Calibration Error (ECE) and Riskâ€“Coverage curve.

---

## Tools & Libraries
- `transformers`, `vllm`, `faiss`, `networkx`
- `rdflib`, `SPARQLWrapper`
- `openai`, `anthropic` (for LLM-judge)
- `pandas`, `matplotlib`, `seaborn`
- `tqdm`, `jsonlines`

---

## Deliverables
1. **Evaluation scripts** (`.py` files under `scripts/`)
2. **Result CSVs** (`summary_metrics.csv`)
3. **Visualizations** (`plots/`)
4. **Benchmark report** summarizing key findings
5. **Appendix:** qualitative examples (correct vs. hallucinated answers)

---

## Example Summary Statement
> Across 500 medical questions, the baseline LLaMA-3-8B model exhibited a hallucination rate of 45%. Incorporating RDF-based retrieval reduced hallucination to 28%, while the Jamba + RDF model achieved 22% â€” a 36â€“50% reduction overall with minimal latency increase. These results demonstrate the effectiveness of structured RDF grounding for mitigating hallucinations in medical NLP systems.

---

